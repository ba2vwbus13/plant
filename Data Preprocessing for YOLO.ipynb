{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Library\n",
    "\n",
    "必要なライブラリを追加する"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import cv2\n",
    "import sys\n",
    "import re\n",
    "import random\n",
    "import numpy as np\n",
    "from PIL import Image, ImageOps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0. Folder Structure Arrangement \n",
    "前処理を行う前に, 下のようにイメージのフォルダー構造を整える必要がある。directory_A, directory_Bは各種類のフォルダーである。\n",
    "\n",
    "注意：　#TODOが見つかった時は実際に名前やパスなどを合わせてください"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "└─/data\n",
    "    └──/directory_A\n",
    "    │    ├── filezz.jpg\n",
    "    │    ├── filezz.txt\n",
    "    │    ├── vidada.jpg\n",
    "    │    └── vidada.txt\n",
    "    └──/directory_B\n",
    "           ├── filett.jpg\n",
    "           ├── filett.txt\n",
    "           ├── nanof.jpg\n",
    "           └── nanof.txt    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: 画像にあるメインフォルダーのパスを変えてください　(~/data)\n",
    "data_path = '/home/ic1/darknet/logs/DCON2021_SenriAi/Southeast_Botanical_Gardens/Plant'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Rename file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of folders : 14\n",
      "0 /home/ic1/darknet/logs/DCON2021_SenriAi/Southeast_Botanical_Gardens/Plant/Dendrobium_Nobile\n",
      ">Total files:17, Renamed files:0, Error: 0\n",
      "1 /home/ic1/darknet/logs/DCON2021_SenriAi/Southeast_Botanical_Gardens/Plant/Oncidium\n",
      ">Total files:50, Renamed files:0, Error: 0\n",
      "2 /home/ic1/darknet/logs/DCON2021_SenriAi/Southeast_Botanical_Gardens/Plant/Nepenthes\n",
      ">Total files:17, Renamed files:0, Error: 0\n",
      "3 /home/ic1/darknet/logs/DCON2021_SenriAi/Southeast_Botanical_Gardens/Plant/Phalaenopsis\n",
      ">Total files:50, Renamed files:0, Error: 0\n",
      "4 /home/ic1/darknet/logs/DCON2021_SenriAi/Southeast_Botanical_Gardens/Plant/DragonFruit\n",
      ">Total files:50, Renamed files:0, Error: 0\n",
      "5 /home/ic1/darknet/logs/DCON2021_SenriAi/Southeast_Botanical_Gardens/Plant/Vanda\n",
      ">Total files:50, Renamed files:0, Error: 0\n",
      "6 /home/ic1/darknet/logs/DCON2021_SenriAi/Southeast_Botanical_Gardens/Plant/Cacao\n",
      ">Total files:13, Renamed files:0, Error: 0\n",
      "7 /home/ic1/darknet/logs/DCON2021_SenriAi/Southeast_Botanical_Gardens/Plant/ParaguaiOni_Bus\n",
      ">Total files:41, Renamed files:0, Error: 0\n",
      "8 /home/ic1/darknet/logs/DCON2021_SenriAi/Southeast_Botanical_Gardens/Plant/Sausage_Tree\n",
      ">Total files:50, Renamed files:0, Error: 0\n",
      "9 /home/ic1/darknet/logs/DCON2021_SenriAi/Southeast_Botanical_Gardens/Plant/Bulbophyllum\n",
      ">Total files:19, Renamed files:0, Error: 0\n",
      "10 /home/ic1/darknet/logs/DCON2021_SenriAi/Southeast_Botanical_Gardens/Plant/Cattleya\n",
      ">Total files:76, Renamed files:0, Error: 0\n",
      "11 /home/ic1/darknet/logs/DCON2021_SenriAi/Southeast_Botanical_Gardens/Plant/Hibiscus\n",
      ">Total files:28, Renamed files:0, Error: 0\n",
      "12 /home/ic1/darknet/logs/DCON2021_SenriAi/Southeast_Botanical_Gardens/Plant/Saihai_Deigo\n",
      ">Total files:47, Renamed files:0, Error: 0\n",
      "13 /home/ic1/darknet/logs/DCON2021_SenriAi/Southeast_Botanical_Gardens/Plant/Tropical_Water_Lily\n",
      ">Total files:50, Renamed files:0, Error: 0\n"
     ]
    }
   ],
   "source": [
    "if os.path.exists(data_path):\n",
    "    input_dirname = os.path.join(data_path, '*')\n",
    "    dirs = [d for d in glob.glob(input_dirname) if os.path.isdir(d)]\n",
    "    print(\"Number of folders : \" + str(len(dirs)))\n",
    "        \n",
    "    for kind, dir in enumerate(dirs):\n",
    "        print(kind, dir)\n",
    "        \n",
    "        files = glob.glob(os.path.join(dir, '*.txt'))\n",
    "        files.sort()\n",
    "        base_name = os.path.basename(dir)\n",
    "        \n",
    "        files_count = len(files)\n",
    "        renamed_count = 0\n",
    "        no_image_count = 0\n",
    "        \n",
    "        for num, file in enumerate(files):\n",
    "            # txtファイルを名前の変更\n",
    "            dname = os.path.dirname(file)\n",
    "            tgt_fname = os.path.join(dname, base_name+str(num+1).zfill(3)+'.txt')\n",
    "            if not os.path.exists(tgt_fname):\n",
    "                os.rename(file, tgt_fname)\n",
    "                renamed_count += 1\n",
    "            \n",
    "            #　イメージの名前の変更\n",
    "            file2 = file.replace('.txt', '.jpg')\n",
    "            if not os.path.exists(file2):\n",
    "                file2 = file.replace('.txt', '.png')\n",
    "                if not os.path.exists(file2):\n",
    "                    file2 = file.replace('.txt', '.jpeg')\n",
    "                    if not os.path.exists(file2):\n",
    "                        file2 = file.replace('.txt', '.JPG')\n",
    "                        if not os.path.exists(file2):\n",
    "                            no_image_count += 1\n",
    "                            continue\n",
    "            tgt_fname2 = tgt_fname.replace('.txt', '.jpg')\n",
    "            if not os.path.exists(tgt_fname2):\n",
    "                os.rename(file2, tgt_fname2)\n",
    "                \n",
    "        print(\">Total files:{}, Renamed files:{}, Error: {}\".format(files_count, renamed_count, no_image_count))\n",
    "else:\n",
    "    print(\"Erro:　Path not found!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Rewrite label (create classes.txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 14 classes:\n",
      "0 : Dendrobium_Nobile\n",
      "1 : Oncidium\n",
      "2 : Nepenthes\n",
      "3 : Phalaenopsis\n",
      "4 : DragonFruit\n",
      "5 : Vanda\n",
      "6 : Cacao\n",
      "7 : ParaguaiOni_Bus\n",
      "8 : Sausage_Tree\n",
      "9 : Bulbophyllum\n",
      "10 : Cattleya\n",
      "11 : Hibiscus\n",
      "12 : Saihai_Deigo\n",
      "13 : Tropical_Water_Lily\n",
      "--------------\n",
      "classes.txt is created at \n",
      "/home/ic1/darknet/logs/DCON2021_SenriAi/Southeast_Botanical_Gardens\n"
     ]
    }
   ],
   "source": [
    "# TODO: fish以外場合はファイル名を変えてください\n",
    "obj_names = \"classes.txt\"\n",
    "label_file = open(obj_names, \"w\")\n",
    "\n",
    "input_dirname = os.path.join(data_path, '*')\n",
    "dirs = [d for d in glob.glob(input_dirname) if os.path.isdir(d)]\n",
    "\n",
    "num_classes = len(dirs)\n",
    "\n",
    "print(\"There are {} classes:\".format(num_classes))\n",
    "\n",
    "for kind, dir in enumerate(dirs):\n",
    "    files = glob.glob(os.path.join(dir, '*.txt'))\n",
    "    label_name = os.path.basename(dir)\n",
    "    label_file.write('{}\\n'.format(label_name))\n",
    "    print(\"{} : {}\".format(kind, label_name))\n",
    "    for file in files:\n",
    "        with open(file, mode='r') as f:\n",
    "            lines = f.readlines()\n",
    "        with open(file, mode='w') as f:\n",
    "            for line in lines:\n",
    "                line2 = line.split()\n",
    "                line2[0] = str(kind)\n",
    "                f.write('{} {} {} {} {}\\n'.format(line2[0], line2[1], line2[2], line2[3], line2[4]))\n",
    "                \n",
    "label_file.close()\n",
    "\n",
    "print(\"--------------\")\n",
    "print(\"{} is created at \\n{}\".format(obj_names, os.getcwd()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Create Augmented Data (Optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-6-98432b8262cf>:47: FutureWarning: Using a non-tuple sequence for multidimensional indexing is deprecated; use `arr[tuple(seq)]` instead of `arr[seq]`. In the future this will be interpreted as an array index, `arr[np.array(seq)]`, which will result either in an error or a different result.\n",
      "  out[coords[:-1]] = (255,255,255)\n",
      "<ipython-input-6-98432b8262cf>:53: FutureWarning: Using a non-tuple sequence for multidimensional indexing is deprecated; use `arr[tuple(seq)]` instead of `arr[seq]`. In the future this will be interpreted as an array index, `arr[np.array(seq)]`, which will result either in an error or a different result.\n",
      "  out[coords[:-1]] = (0,0,0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Augmented data created\n",
      "Location: /home/ic1/darknet/logs/DCON2021_SenriAi/Southeast_Botanical_Gardens/augmentedSenriai\n"
     ]
    }
   ],
   "source": [
    "#TODO augmented no folder wo atarasiku tukuttene\n",
    "original_dir = data_path  \n",
    "aug_path = os.path.join(os.path.dirname(data_path), 'augmentedSenriai') \n",
    "\n",
    "if not os.path.exists(aug_path):\n",
    "        os.makedirs(aug_path) \n",
    "        \n",
    "class Augmentation:\n",
    "    def __init__(self, original_dir, aug_path):\n",
    "        self.original_dir = original_dir\n",
    "        self.aug_path = aug_path\n",
    "    \n",
    "    # ヒストグラム均一化\n",
    "    def equalizeHistRGB(self, src):\n",
    "        RGB = cv2.split(src)\n",
    "        Blue   = RGB[0]\n",
    "        Green = RGB[1]\n",
    "        Red    = RGB[2]\n",
    "        for i in range(3):\n",
    "            cv2.equalizeHist(RGB[i])\n",
    "\n",
    "        img_hist = cv2.merge([RGB[0],RGB[1], RGB[2]])\n",
    "        return img_hist\n",
    "\n",
    "    # ガウシアンノイズ\n",
    "    def addGaussianNoise(self, src):\n",
    "        row,col,ch= src.shape\n",
    "        mean = 0\n",
    "        var = 0.1\n",
    "        sigma = 15\n",
    "        gauss = np.random.normal(mean,sigma,(row,col,ch))\n",
    "        gauss = gauss.reshape(row,col,ch)\n",
    "        noisy = src + gauss\n",
    "\n",
    "        return noisy\n",
    "\n",
    "    # salt&pepperノイズ\n",
    "    def addSaltPepperNoise(self, src):\n",
    "        row,col,ch = src.shape\n",
    "        s_vs_p = 0.5\n",
    "        amount = 0.004\n",
    "        out = src.copy()\n",
    "        # Salt mode\n",
    "        num_salt = np.ceil(amount * src.size * s_vs_p)\n",
    "        coords = [np.random.randint(0, i-1 , int(num_salt))\n",
    "                  for i in src.shape]\n",
    "        out[coords[:-1]] = (255,255,255)\n",
    "\n",
    "        # Pepper mode\n",
    "        num_pepper = np.ceil(amount* src.size * (1. - s_vs_p))\n",
    "        coords = [np.random.randint(0, i-1 , int(num_pepper))\n",
    "                  for i in src.shape]\n",
    "        out[coords[:-1]] = (0,0,0)\n",
    "        return out\n",
    "\n",
    "    def main(self, target_dir, aug_path):\n",
    "        # ルックアップテーブルの生成\n",
    "        min_table = 50\n",
    "        max_table = 205\n",
    "        diff_table = max_table - min_table\n",
    "        gamma1 = 0.75\n",
    "        gamma2 = 1.5\n",
    "\n",
    "        LUT_HC = np.arange(256, dtype = 'uint8' )\n",
    "        LUT_LC = np.arange(256, dtype = 'uint8' )\n",
    "        LUT_G1 = np.arange(256, dtype = 'uint8' )\n",
    "        LUT_G2 = np.arange(256, dtype = 'uint8' )\n",
    "\n",
    "        LUTs = []\n",
    "\n",
    "        # 平滑化用\n",
    "        average_square = (10,10)\n",
    "\n",
    "        # ハイコントラストLUT作成\n",
    "        for i in range(0, min_table):\n",
    "            LUT_HC[i] = 0\n",
    "\n",
    "        for i in range(min_table, max_table):\n",
    "            LUT_HC[i] = 255 * (i - min_table) / diff_table\n",
    "\n",
    "        for i in range(max_table, 255):\n",
    "            LUT_HC[i] = 255\n",
    "\n",
    "        # その他LUT作成\n",
    "        for i in range(256):\n",
    "            LUT_LC[i] = min_table + i * (diff_table) / 255\n",
    "            LUT_G1[i] = 255 * pow(float(i) / 255, 1.0 / gamma1)\n",
    "            LUT_G2[i] = 255 * pow(float(i) / 255, 1.0 / gamma2)\n",
    "\n",
    "        LUTs.append(LUT_HC)\n",
    "        LUTs.append(LUT_LC)\n",
    "        LUTs.append(LUT_G1)\n",
    "        LUTs.append(LUT_G2)\n",
    "\n",
    "        # 画像の読み込み\n",
    "        files = glob.glob(target_dir+'/*.jpg')\n",
    "\n",
    "        for item_index, file in enumerate(files):\n",
    "            img_src = cv2.imread(file, 1)\n",
    "\n",
    "            trans_img = []\n",
    "            trans_img.append(img_src)\n",
    "\n",
    "            # LUT変換\n",
    "            for i, LUT in enumerate(LUTs):\n",
    "                trans_img.append( cv2.LUT(img_src, LUT))\n",
    "\n",
    "            # 平滑化\n",
    "            trans_img.append(cv2.blur(img_src, average_square))\n",
    "\n",
    "            # ヒストグラム均一化\n",
    "            trans_img.append(self.equalizeHistRGB(img_src))\n",
    "\n",
    "            # ノイズ付加\n",
    "            trans_img.append(self.addGaussianNoise(img_src))\n",
    "            trans_img.append(self.addSaltPepperNoise(img_src))\n",
    "\n",
    "            # 反転\n",
    "            flip_img = []\n",
    "            for img in trans_img:\n",
    "                flip_img.append(cv2.flip(img, 1))\n",
    "            trans_img.extend(flip_img)\n",
    "\n",
    "            # 保存\n",
    "            if not os.path.exists(aug_path):\n",
    "                os.mkdir(aug_path)\n",
    "\n",
    "            base = os.path.splitext(os.path.basename(file))[0] + \"_\"\n",
    "            item_txt = os.path.splitext(os.path.basename(file))[0] + '.txt'\n",
    "            img_src.astype(np.float64)\n",
    "            for i, img in enumerate(trans_img):\n",
    "                # 比較用\n",
    "                # cv2.imwrite(\"target_images/\" + base + str(i) + \".jpg\" ,cv2.hconcat([img_src.astype(np.float64), img.astype(np.float64)]))\n",
    "                #9以下は反転していない\n",
    "                coordinate = ''\n",
    "                #file = open(\"target_images/\" + item_txt, 'r')\n",
    "                file = open(target_dir+\"/\" + item_txt, 'r')\n",
    "                if i <= 8:\n",
    "                    coordinate = file.read()\n",
    "                #9以上は反転している\n",
    "                else:                    \n",
    "                    while True:\n",
    "                        line = file.readline()\n",
    "                        if not line:\n",
    "                            break\n",
    "                        words = re.split(\" +\", line)\n",
    "                        if len(words) >= 2:\n",
    "                            label=words[0]\n",
    "                            x = words[1]\n",
    "                            y = words[2]\n",
    "                            width = words[3]\n",
    "                            height = words[4]\n",
    "                            x = str(1.0 - float(x))\n",
    "                            coordinate = coordinate + ' '.join([\n",
    "                                str(label).strip(),\n",
    "                                str(x).strip(),\n",
    "                                str(y).strip(),\n",
    "                                str(width).strip(),\n",
    "                                str(height).strip()\n",
    "                            ]) + '\\n'\n",
    "\n",
    "                file.close()\n",
    "                fn = aug_path + \"/\" + base + str(i) + \".txt\"\n",
    "                with open(fn, 'w') as f:\n",
    "                    f.write(coordinate)\n",
    "                cv2.imwrite(aug_path + \"/\" + base + str(i) + \".jpg\" ,img)\n",
    "\n",
    "    def add_noise(self):\n",
    "        if not os.path.exists(self.aug_path):\n",
    "            os.mkdir(self.aug_path)        \n",
    "        drs=glob.glob(self.original_dir+'/*')\n",
    "        if os.path.isfile(drs[0]):\n",
    "            self.main(self.original_dir, self.aug_path)\n",
    "        else:\n",
    "            for dr in drs:\n",
    "                la = os.path.basename(dr)\n",
    "                inc_sub_dr = os.path.join(self.aug_path, la)\n",
    "                if not os.path.exists(inc_sub_dr):\n",
    "                    os.mkdir(inc_sub_dr)\n",
    "                self.main(dr, inc_sub_dr)\n",
    "\n",
    "data = Augmentation(original_dir, aug_path)\n",
    "data.add_noise()\n",
    "\n",
    "print(\"Augmented data created\")\n",
    "print(\"Location: {}\".format(aug_path))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Create Negative Data (Optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: ネガティブ画像のフォルダーパスを変えてください\n",
    "negative_input_dirname = '/Users/davidpich/Desktop/negative'\n",
    "\n",
    "if not os.path.exists(negative_input_dirname):\n",
    "    print(\"Error: Folder not found! Please enter a valid path\")\n",
    "\n",
    "negative_basename = os.path.dirname(negative_input_dirname)\n",
    "negative_output_dirname = os.path.join(negative_basename, 'negative_data')\n",
    "\n",
    "BaseW = 416\n",
    "BaseH = 416\n",
    "\n",
    "reshaped_size = (BaseW, BaseH)\n",
    "\n",
    "if not os.path.exists(negative_output_dirname):\n",
    "        os.makedirs(negative_output_dirname) \n",
    "files = glob.glob(os.path.join(negative_input_dirname, '*'))\n",
    "\n",
    "for i, file in enumerate(files):\n",
    "    index = i + 1\n",
    "    try:\n",
    "        image = Image.open(file)\n",
    "        image.thumbnail(reshaped_size)\n",
    "        converted = image.convert('RGB')\n",
    "\n",
    "        save_fl = os.path.join(negative_output_dirname, 'negative'+str(index)+'.jpg')\n",
    "        save_fl_ano = save_fl.replace('.jpg', '.txt') \n",
    "        \n",
    "        image.save(save_fl)\n",
    "        \n",
    "        with open(save_fl_ano, mode='w') as f:\n",
    "            f.write('')\n",
    "        \n",
    "#         \"\"\"\n",
    "#         細かい画像を増やしたい場合\n",
    "#         \"\"\"\n",
    "#         #image divider\n",
    "#         div_x, div_y = converted.size\n",
    "#         div_x, div_y = div_x//3, div_y//3\n",
    "\n",
    "#         for k in range(2):\n",
    "#             for m in range(2):\n",
    "#                 img = converted.crop((m*div_x, k*div_y, (m+1)*div_x, (k+1)*div_y))\n",
    "#                 save_fl = nagative_output_dirname+'/'+str(index)+'_'+str(k)+'_'+str(m)+'.jpg'\n",
    "#                 img.save(save_fl)\n",
    "#                 save_fl_ano = save_fl.replace('.jpg', '.txt') #アノテーションファイル名\n",
    "#                 with open(save_fl_ano, mode='w') as f:\n",
    "#                     f.write('')\n",
    "        \n",
    "    except IOError:\n",
    "        pass\n",
    "    \n",
    "print(\"Negative data created.\")\n",
    "print(\"Location: {}\".format(negative_output_dirname))/augmented"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Split Dataset (Training + Testing)\n",
    "\n",
    "以下に1つ方法を選んでください (create yolo_train.txt + yolo_test.txt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1 Original Data Only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset size: 1\n",
      "Training data :1\n",
      "Validation data :0\n"
     ]
    }
   ],
   "source": [
    "random.seed(42)\n",
    "\n",
    "dataset_fn = 'dataset.txt'\n",
    "train_fn = 'yolo_train.txt'\n",
    "test_fn = 'yolo_test.txt'\n",
    "\n",
    "dataset_f = open(dataset_fn, 'w')\n",
    "positive_output_dirname =  data_path\n",
    "\n",
    "drs=glob.glob(positive_output_dirname+'/*')\n",
    "drs.sort()\n",
    "for dr in drs:\n",
    "    if os.path.isfile(dr):\n",
    "        continue\n",
    "    fls = glob.glob(dr+'/*.jpg')\n",
    "    for fl in fls:\n",
    "        try:\n",
    "            dataset_f.write('{}\\n'.format(os.path.abspath(fl)))\n",
    "        except:\n",
    "            pass\n",
    "    label_name = os.path.basename(dr)\n",
    "dataset_f.close()\n",
    "\n",
    "with open(dataset_fn,mode='r') as f:\n",
    "    ln = f.readlines()\n",
    "\n",
    "separate_ratio = 0.1\n",
    "num_test = (int)(len(ln)*separate_ratio)\n",
    "num_train = len(ln)-num_test\n",
    "\n",
    "random.shuffle(ln)\n",
    "test_data = ln[:num_test]\n",
    "train_data = ln[num_test:]\n",
    "\n",
    "with open(train_fn, 'w') as f:\n",
    "    f.writelines(train_data)\n",
    "    \n",
    "with open(test_fn, 'w') as f:\n",
    "    f.writelines(test_data)\n",
    "\n",
    "print(\"Dataset size: \" + str(len(ln)))\n",
    "print(\"Training data :\"+ str(len(train_data)))\n",
    "print(\"Validation data :\"+ str(len(test_data)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 Original Data + Negative Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(42)\n",
    "\n",
    "dataset_fn = 'dataset.txt'\n",
    "train_fn = 'yolo_train.txt'\n",
    "test_fn = 'yolo_test.txt'\n",
    "\n",
    "dataset_f = open(dataset_fn, 'w')\n",
    "positive_output_dirname =  data_path\n",
    "\n",
    "drs=glob.glob(positive_output_dirname+'/*')\n",
    "drs.sort()\n",
    "for dr in drs:\n",
    "    if os.path.isfile(dr):\n",
    "        continue\n",
    "    fls = glob.glob(dr+'/*.jpg')\n",
    "    for fl in fls:\n",
    "        try:\n",
    "            dataset_f.write('{}\\n'.format(os.path.abspath(fl)))\n",
    "        except:\n",
    "            pass\n",
    "    label_name = os.path.basename(dr)\n",
    "dataset_f.close()\n",
    "\n",
    "with open(dataset_fn,mode='r') as f:\n",
    "    ln = f.readlines()\n",
    "\n",
    "separate_ratio = 0.1\n",
    "num_test = (int)(len(ln)*separate_ratio)\n",
    "num_train = len(ln)-num_test\n",
    "\n",
    "random.shuffle(ln)\n",
    "test_data = ln[:num_test]\n",
    "train_data = ln[num_test:]\n",
    "\n",
    "negative_data = []\n",
    "files=glob.glob(negative_output_dirname+'/*.jpg')\n",
    "for i,file in enumerate(files):\n",
    "    negative_data.append(os.path.abspath(file)+'\\n')\n",
    "\n",
    "random.shuffle(negative_data)\n",
    "\n",
    "#どれくらいの割合でネガティブデータを利用するか\n",
    "netative_data_ratio = 1 \n",
    "negative_data = negative_data[:(int)(len(negative_data)*netative_data_ratio)]\n",
    "\n",
    "train_data = train_data + negative_data\n",
    "random.shuffle(train_data)\n",
    "\n",
    "with open(test_fn, 'w') as f:\n",
    "    f.writelines(test_data)\n",
    "with open(train_fn, 'w') as f:\n",
    "    f.writelines(train_data)\n",
    "    \n",
    "print(\"Dataset size: \" + str(len(ln)))\n",
    "print(\"Negative data added: \" + str(len(negative_data)))\n",
    "print(\"Training data :\"+ str(len(train_data)))\n",
    "print(\"Validation data :\"+ str(len(test_data)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3 Augmented Data Only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset size: 10044\n",
      "Training data :9040\n",
      "Validation data :1004\n"
     ]
    }
   ],
   "source": [
    "random.seed(42)\n",
    "\n",
    "dataset_fn = 'dataset.txt'\n",
    "train_fn = 'yolo_train.txt'\n",
    "test_fn = 'yolo_test.txt'\n",
    "\n",
    "dataset_f = open(dataset_fn, 'w')\n",
    "positive_output_dirname =  aug_path\n",
    "\n",
    "drs=glob.glob(positive_output_dirname+'/*')\n",
    "drs.sort()\n",
    "for dr in drs:\n",
    "    if os.path.isfile(dr):\n",
    "        continue\n",
    "    fls = glob.glob(dr+'/*.jpg')\n",
    "    for fl in fls:\n",
    "        try:\n",
    "            dataset_f.write('{}\\n'.format(os.path.abspath(fl)))\n",
    "        except:\n",
    "            pass\n",
    "    label_name = os.path.basename(dr)\n",
    "dataset_f.close()\n",
    "\n",
    "with open(dataset_fn,mode='r') as f:\n",
    "    ln = f.readlines()\n",
    "\n",
    "separate_ratio = 0.1\n",
    "num_test = (int)(len(ln)*separate_ratio)\n",
    "num_train = len(ln)-num_test\n",
    "\n",
    "random.shuffle(ln)\n",
    "test_data = ln[:num_test]\n",
    "train_data = ln[num_test:]\n",
    "\n",
    "with open(train_fn, 'w') as f:\n",
    "    f.writelines(train_data)\n",
    "    \n",
    "with open(test_fn, 'w') as f:\n",
    "    f.writelines(test_data)\n",
    "\n",
    "print(\"Dataset size: \" + str(len(ln)))\n",
    "print(\"Training data :\"+ str(len(train_data)))\n",
    "print(\"Validation data :\"+ str(len(test_data)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.4 Augmented Data + Negative Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(42)\n",
    "\n",
    "dataset_fn = 'dataset.txt'\n",
    "train_fn = 'yolo_train.txt'\n",
    "test_fn = 'yolo_test.txt'\n",
    "\n",
    "dataset_f = open(dataset_fn, 'w')\n",
    "positive_output_dirname =  aug_path\n",
    "\n",
    "drs=glob.glob(positive_output_dirname+'/*')\n",
    "drs.sort()\n",
    "for dr in drs:\n",
    "    if os.path.isfile(dr):\n",
    "        continue\n",
    "    fls = glob.glob(dr+'/*.jpg')\n",
    "    for fl in fls:\n",
    "        try:\n",
    "            dataset_f.write('{}\\n'.format(os.path.abspath(fl)))\n",
    "        except:\n",
    "            pass\n",
    "    label_name = os.path.basename(dr)\n",
    "dataset_f.close()\n",
    "\n",
    "with open(dataset_fn,mode='r') as f:\n",
    "    ln = f.readlines()\n",
    "\n",
    "separate_ratio = 0.1\n",
    "num_test = (int)(len(ln)*separate_ratio)\n",
    "num_train = len(ln)-num_test\n",
    "\n",
    "random.shuffle(ln)\n",
    "test_data = ln[:num_test]\n",
    "train_data = ln[num_test:]\n",
    "\n",
    "negative_data = []\n",
    "files=glob.glob(negative_output_dirname+'/*.jpg')\n",
    "for i,file in enumerate(files):\n",
    "    negative_data.append(os.path.abspath(file)+'\\n')\n",
    "\n",
    "random.shuffle(negative_data)\n",
    "\n",
    "#どれくらいの割合でネガティブデータを利用するか\n",
    "netative_data_ratio = 1 \n",
    "negative_data = negative_data[:(int)(len(negative_data)*netative_data_ratio)]\n",
    "\n",
    "train_data = train_data + negative_data\n",
    "random.shuffle(train_data)\n",
    "\n",
    "with open(test_fn, 'w') as f:\n",
    "    f.writelines(test_data)\n",
    "with open(train_fn, 'w') as f:\n",
    "    f.writelines(train_data)\n",
    "    \n",
    "print(\"Dataset size: \" + str(len(ln)))\n",
    "print(\"Negative data added: \" + str(len(negative_data)))\n",
    "print(\"Training data :\"+ str(len(train_data)))\n",
    "print(\"Validation data :\"+ str(len(test_data)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Generate data file (create fish.data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "senriai.data created\n",
      "Location: /home/ic1/darknet/logs/DCON2021_SenriAi/Southeast_Botanical_Gardens/senriai.data\n"
     ]
    }
   ],
   "source": [
    "# TODO: fish以外場合はファイル名を変えてください\n",
    "filename = \"senriai.data\"\n",
    "obj_data_path = os.path.join(os.getcwd(), filename)\n",
    "\n",
    "classes = num_classes\n",
    "train_path = os.path.join(os.getcwd(), train_fn)\n",
    "test_path = os.path.join(os.getcwd(), test_fn)\n",
    "name_path = os.path.join(os.getcwd(), obj_names)\n",
    "backup = 'backup'\n",
    "backup_path = os.path.join(os.getcwd(), backup)\n",
    "\n",
    "if not os.path.exists(backup_path):\n",
    "    os.makedirs(backup_path)\n",
    "\n",
    "with open(filename, 'w') as f:\n",
    "    f.write(\"classes = {}\\ntrain = {}\\nvalid = {}\\nnames = {}\\nbackup ={}\".\n",
    "            format(classes, train_path, test_path, name_path, backup_path))\n",
    "    \n",
    "print(\"{} created\".format(filename))\n",
    "print(\"Location: {}\".format(obj_data_path))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. Generate README (create README.txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "README file created\n",
      "Location: /home/ic1/darknet/logs/DCON2021_SenriAi/Southeast_Botanical_Gardens\n"
     ]
    }
   ],
   "source": [
    "# TODO: 実際のファイル名に合わせてください\n",
    "import os\n",
    "filename = \"senriai.data\"\n",
    "obj_data_path = os.path.join(os.getcwd(), filename)\n",
    "backup = 'backup'\n",
    "pretrained_file = 'darknet53.conv.74'\n",
    "cfg_file = 'yolov4.cfg'\n",
    "video_path = 'test1.mp4'\n",
    "log_file = \"log.txt\"\n",
    "\n",
    "backup_path = os.path.join(os.getcwd(), backup)\n",
    "cfg_path = os.path.join(os.getcwd(), cfg_file)\n",
    "log_path = os.path.join(os.getcwd(), log_file)\n",
    "weight_path = os.path.join(backup_path, cfg_file[:-4]+'_last.weight')\n",
    "\n",
    "training_cmd = \"./darknet detector train {} {} {} -map\".format(obj_data_path, cfg_path, pretrained_file)\n",
    "training_str = \"[For training]\\n{}\\n\".format(training_cmd)\n",
    "\n",
    "training_with_log_str = \"[For Training with log]\\n{} | grep \\\"avg loss\\\" | tee {}\\n\".format(training_cmd, log_path) \n",
    "\n",
    "prediction_cmd = \"./darknet detector demo {} {} {} {}\".format(obj_data_path, cfg_path, weight_path, video_path)\n",
    "prediction_str = \"[For video prediction]\\n{}\\n\".format(prediction_cmd)\n",
    "\n",
    "evaluation_cmd = \"./darknet detector map {} {} {} -iou_thresh 0.75\".format(obj_data_path, cfg_path, weight_path)\n",
    "evaluation_str = \"[For evaluation mAP]\\n{}\\n\".format(evaluation_cmd)\n",
    "\n",
    "with open(\"README.txt\", \"w\") as f:\n",
    "    f.write(\"{}\\n{}\\n{}\\n{}\".format(training_str, training_with_log_str, prediction_str, evaluation_str))\n",
    "    \n",
    "print(\"README file created\")\n",
    "print(\"Location: {}\".format(os.getcwd()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
